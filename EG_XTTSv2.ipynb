{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports + Git"
      ],
      "metadata": {
        "id": "p0MQd_i1HGp_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KoIzNXnRNCL4"
      },
      "outputs": [],
      "source": [
        "# prompt: ERROR: file:///content/TTS/TTS/tts/layers/xtts does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found\n",
        "\n",
        "!git clone https://github.com/coqui-ai/TTS\n",
        "%cd TTS\n",
        "!pip install -e .[all,dev,notebooks]  # Select the relevant extras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsg7E7VvQ41p"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lBsVlvhQCpO"
      },
      "source": [
        "Testing tokonizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neueFOpvOCt1"
      },
      "outputs": [],
      "source": [
        "from TTS.TTS.tts.layers.xtts.tokenizer import VoiceBpeTokenizer\n",
        "\n",
        "# Path to the vocab.json file\n",
        "vocab_path = \"/content/vocab.json\"\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = VoiceBpeTokenizer(vocab_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJXI9H8sOTfm",
        "outputId": "42c1c99d-10dd-47c2-d20c-37ff9864af8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Output: [5022, 3961, 4066, 3953, 2, 4268, 4105]\n"
          ]
        }
      ],
      "source": [
        "# Text to tokenize\n",
        "text = \"صباح الخير\"\n",
        "\n",
        "# Tokenize the text (encode it)\n",
        "tokens = tokenizer.encode(text, lang=\"ar\")\n",
        "\n",
        "# Print tokenized output\n",
        "print(\"Tokenized Output:\", tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Dataset from Google drive"
      ],
      "metadata": {
        "id": "sRNyvnCdHLlS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e383J1qUQFKG"
      },
      "source": [
        "downloading dataset from google drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "id": "kzqhK_5jLlVH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b22dca57-b3bb-4cb5-f120-00e47ac3485f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1-EB55BAs92xLQa_sM3PptWZ6xlJ6yZhY"
      ],
      "metadata": {
        "id": "X_doWh_bLkqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SevNM8-gQKOk",
        "outputId": "75db3f87-3f19-4862-80fa-35250d5c36a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File successfully unzipped to /content/lJspeech\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Create the target directory\n",
        "target_dir = '/content/lJspeech'\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(\"/content/txt_to_speech_dataset.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(target_dir)\n",
        "\n",
        "print(f'File successfully unzipped to {target_dir}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Config + Training"
      ],
      "metadata": {
        "id": "E3EsdsLQHYUc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb5Qtm0fFLpy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from trainer import Trainer, TrainerArgs\n",
        "\n",
        "from TTS.config.shared_configs import BaseDatasetConfig\n",
        "from TTS.tts.datasets import load_tts_samples\n",
        "from TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig\n",
        "from TTS.utils.manage import ModelManager\n",
        "\n",
        "# Logging parameters\n",
        "RUN_NAME = \"GPT_XTTS_v2.0_LJSpeech_FT\"\n",
        "PROJECT_NAME = \"XTTS_trainer\"\n",
        "DASHBOARD_LOGGER = \"tensorboard\"\n",
        "LOGGER_URI = None\n",
        "\n",
        "# Set here the path that the checkpoints will be saved. Default: ./run/training/\n",
        "OUT_PATH = \"/content/run/training/\"\n",
        "\n",
        "# Training Parameters\n",
        "OPTIMIZER_WD_ONLY_ON_WEIGHTS = True  # for multi-gpu training please make it False\n",
        "START_WITH_EVAL = True  # if True it will star with evaluation\n",
        "BATCH_SIZE = 3  # set here the batch size\n",
        "GRAD_ACUMM_STEPS = 84  # set here the grad accumulation steps\n",
        "# Note: we recommend that BATCH_SIZE * GRAD_ACUMM_STEPS need to be at least 252 for more efficient training. You can increase/decrease BATCH_SIZE but then set GRAD_ACUMM_STEPS accordingly.\n",
        "\n",
        "# Define here the dataset that you want to use for the fine-tuning on.\n",
        "config_dataset = BaseDatasetConfig(\n",
        "    formatter=\"ljspeech\",\n",
        "    dataset_name=\"ljspeech\",\n",
        "    path=\"/content/lJspeech/\",\n",
        "    meta_file_train=\"/content/lJspeech/metadata.csv\",\n",
        "    language=\"ar\",\n",
        ")\n",
        "\n",
        "# Add here the configs of the datasets\n",
        "DATASETS_CONFIG_LIST = [config_dataset]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HChF56YIQvJL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the path where XTTS v2.0.1 files will be downloaded\n",
        "CHECKPOINTS_OUT_PATH = os.path.join(OUT_PATH, \"XTTS_v2.0_original_model_files/\")\n",
        "os.makedirs(CHECKPOINTS_OUT_PATH, exist_ok=True)\n",
        "\n",
        "\n",
        "# DVAE files\n",
        "DVAE_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth\"\n",
        "MEL_NORM_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth\"\n",
        "\n",
        "# Set the path to the downloaded files\n",
        "DVAE_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(DVAE_CHECKPOINT_LINK))\n",
        "MEL_NORM_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(MEL_NORM_LINK))\n",
        "\n",
        "# download DVAE files if needed\n",
        "if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n",
        "    print(\" > Downloading DVAE files!\")\n",
        "    ModelManager._download_model_files([MEL_NORM_LINK, DVAE_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=False)\n",
        "\n",
        "\n",
        "# Download XTTS v2.0 checkpoint if needed\n",
        "TOKENIZER_FILE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json\"\n",
        "XTTS_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth\"\n",
        "\n",
        "# XTTS transfer learning parameters: You we need to provide the paths of XTTS model checkpoint that you want to do the fine tuning.\n",
        "TOKENIZER_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(TOKENIZER_FILE_LINK))  # vocab.json file\n",
        "XTTS_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(XTTS_CHECKPOINT_LINK))  # model.pth file\n",
        "\n",
        "# download XTTS v2.0 files if needed\n",
        "if not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):\n",
        "    print(\" > Downloading XTTS v2.0 files!\")\n",
        "    ModelManager._download_model_files(\n",
        "        [TOKENIZER_FILE_LINK, XTTS_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=False\n",
        "    )\n",
        "\n",
        "\n",
        "# Training sentences generations\n",
        "SPEAKER_REFERENCE = [\n",
        "    \"/content/lJspeech/wavs/audio_10302.wav\"  # speaker reference to be used in training test sentences\n",
        "]\n",
        "LANGUAGE = config_dataset.language\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YV2M826XRT5e"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # init args and config\n",
        "    model_args = GPTArgs(\n",
        "        max_conditioning_length=132300,  # 6 secs\n",
        "        min_conditioning_length=66150,  # 3 secs\n",
        "        debug_loading_failures=False,\n",
        "        max_wav_length=255995,  # ~11.6 seconds\n",
        "        max_text_length=200,\n",
        "        mel_norm_file=MEL_NORM_FILE,\n",
        "        dvae_checkpoint=DVAE_CHECKPOINT,\n",
        "        xtts_checkpoint=XTTS_CHECKPOINT,  # checkpoint path of the model that you want to fine-tune\n",
        "        tokenizer_file=TOKENIZER_FILE,\n",
        "        gpt_num_audio_tokens=1026,\n",
        "        gpt_start_audio_token=1024,\n",
        "        gpt_stop_audio_token=1025,\n",
        "        gpt_use_masking_gt_prompt_approach=True,\n",
        "        gpt_use_perceiver_resampler=True,\n",
        "    )\n",
        "    # define audio config\n",
        "    audio_config = XttsAudioConfig(sample_rate=44100, dvae_sample_rate=22050, output_sample_rate=24000)\n",
        "    # training parameters config\n",
        "    config = GPTTrainerConfig(\n",
        "        output_path=OUT_PATH,\n",
        "        model_args=model_args,\n",
        "        run_name=RUN_NAME,\n",
        "        project_name=PROJECT_NAME,\n",
        "        run_description=\"\"\"\n",
        "            GPT XTTS training\n",
        "            \"\"\",\n",
        "        dashboard_logger=DASHBOARD_LOGGER,\n",
        "        logger_uri=LOGGER_URI,\n",
        "        audio=audio_config,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        batch_group_size=48,\n",
        "        eval_batch_size=BATCH_SIZE,\n",
        "        num_loader_workers=8,\n",
        "        eval_split_max_size=256,\n",
        "        print_step=50,\n",
        "        plot_step=100,\n",
        "        log_model_step=1000,\n",
        "        save_step=10000,\n",
        "        save_n_checkpoints=1,\n",
        "        save_checkpoints=True,\n",
        "        # target_loss=\"loss\",\n",
        "        print_eval=False,\n",
        "        # Optimizer values like tortoise, pytorch implementation with modifications to not apply WD to non-weight parameters.\n",
        "        optimizer=\"AdamW\",\n",
        "        optimizer_wd_only_on_weights=OPTIMIZER_WD_ONLY_ON_WEIGHTS,\n",
        "        optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n",
        "        lr=5e-06,  # learning rate\n",
        "        lr_scheduler=\"MultiStepLR\",\n",
        "        # it was adjusted accordly for the new step scheme\n",
        "        lr_scheduler_params={\"milestones\": [50000 * 18, 150000 * 18, 300000 * 18], \"gamma\": 0.5, \"last_epoch\": -1},\n",
        "        test_sentences=[\n",
        "            {\n",
        "                \"text\": \"صباح الخير انا بتكلم مصرى عادى\",\n",
        "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
        "                \"language\": LANGUAGE,\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"اهلا انا محمود و دا المشروع\",\n",
        "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
        "                \"language\": LANGUAGE,\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # init the model from config\n",
        "    model = GPTTrainer.init_from_config(config)\n",
        "\n",
        "    # load training samples\n",
        "    train_samples, eval_samples = load_tts_samples(\n",
        "        DATASETS_CONFIG_LIST,\n",
        "        eval_split=True,\n",
        "        eval_split_max_size=config.eval_split_max_size,\n",
        "        eval_split_size=config.eval_split_size,\n",
        "    )\n",
        "\n",
        "    # init the trainer and 🚀\n",
        "    trainer = Trainer(\n",
        "        TrainerArgs(\n",
        "            restore_path=None,  # xtts checkpoint is restored via xtts_checkpoint key so no need of restore it using Trainer restore_path parameter\n",
        "            skip_train_epoch=False,\n",
        "            start_with_eval=START_WITH_EVAL,\n",
        "            grad_accum_steps=GRAD_ACUMM_STEPS,\n",
        "        ),\n",
        "        config,\n",
        "        output_path=OUT_PATH,\n",
        "        model=model,\n",
        "        train_samples=train_samples,\n",
        "        eval_samples=eval_samples,\n",
        "    )\n",
        "    trainer.fit()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uploading Training weights to drive"
      ],
      "metadata": {
        "id": "DQR2n4IOHjCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save this file in google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "!mkdir -p /content/drive/MyDrive/weights/\n",
        "#!cp -r /content/txt_to_speech_dataset /content/drive/MyDrive/txt_to_speech_dataset_files\n",
        "!cp /content/run/training/GPT_XTTS_v2.0_LJSpeech_FT-January-05-2025_06+50AM-0000000/best_model.pth /content/drive/MyDrive/weights/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsOlrinV7cwO",
        "outputId": "0137732f-2276-4115-b51d-a30c1ff5014a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Inference"
      ],
      "metadata": {
        "id": "y9eCNtxyHuC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Finetuned_model\n",
        "!gdown --id 1-131EwOwmJtgC5tg0B9CiGOpN0f1yOGz #checkpoint.pth\n",
        "!gdown --id 1I38OqwNPx7ikwi-MoRhPjklUBJS-Q20d #config.json"
      ],
      "metadata": {
        "id": "MBu6btv5JBcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original Model"
      ],
      "metadata": {
        "id": "jL8lCk_5I3N7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "from TTS.tts.configs.xtts_config import XttsConfig\n",
        "from TTS.tts.models.xtts import Xtts\n",
        "\n",
        "# Add here the xtts_config path\n",
        "CONFIG_PATH = \"/content/run/training/GPT_XTTS_v2.0_LJSpeech_FT-January-05-2025_06+50AM-0000000/config.json\"\n",
        "# Add here the vocab file that you have used to train the model\n",
        "TOKENIZER_PATH = \"/content/run/training/XTTS_v2.0_original_model_files/vocab.json\"\n",
        "# Add here the checkpoint that you want to do inference with\n",
        "XTTS_CHECKPOINT = \"/content/run/training/XTTS_v2.0_original_model_files/model.pth\"\n",
        "# Add here the speaker reference\n",
        "SPEAKER_REFERENCE = \"/content/lJspeech/wavs/audio_10302.wav\"\n",
        "\n",
        "# output wav path\n",
        "OUTPUT_WAV_PATH = \"Original.wav\"\n",
        "\n",
        "print(\"Loading model...\")\n",
        "config = XttsConfig()\n",
        "config.load_json(CONFIG_PATH)\n",
        "model = Xtts.init_from_config(config)\n",
        "model.load_checkpoint(config, checkpoint_path=XTTS_CHECKPOINT, vocab_path=TOKENIZER_PATH, use_deepspeed=False)\n",
        "model.cuda()\n",
        "\n",
        "print(\"Computing speaker latents...\")\n",
        "gpt_cond_latent, speaker_embedding = model.get_conditioning_latents(audio_path=[SPEAKER_REFERENCE])\n",
        "\n"
      ],
      "metadata": {
        "id": "ayx5E1GcZ3Cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26822eb7-f535-467b-e3da-4498396f88d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/TTS/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(f, map_location=map_location, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing speaker latents...\n",
            "Inference...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuned Model"
      ],
      "metadata": {
        "id": "-esr6AdWI51n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "from TTS.tts.configs.xtts_config import XttsConfig\n",
        "from TTS.tts.models.xtts import Xtts\n",
        "\n",
        "# Add here the xtts_config path\n",
        "CONFIG_PATH = \"/content/run/training/GPT_XTTS_v2.0_LJSpeech_FT-January-05-2025_06+50AM-0000000/config.json\"\n",
        "# Add here the vocab file that you have used to train the model\n",
        "TOKENIZER_PATH = \"/content/run/training/XTTS_v2.0_original_model_files/vocab.json\"\n",
        "# Add here the checkpoint that you want to do inference with\n",
        "XTTS_CHECKPOINT = \"/content/run/training/GPT_XTTS_v2.0_LJSpeech_FT-January-05-2025_06+50AM-0000000/checkpoint_18347.pth\"\n",
        "# Add here the speaker reference\n",
        "SPEAKER_REFERENCE = \"/content/lJspeech/wavs/audio_10302.wav\"\n",
        "\n",
        "# output wav path\n",
        "OUTPUT_WAV_PATH1 = \"Finetuned.wav\"\n",
        "\n",
        "print(\"Loading model...\")\n",
        "config = XttsConfig()\n",
        "config.load_json(CONFIG_PATH)\n",
        "model1 = Xtts.init_from_config(config)\n",
        "model1.load_checkpoint(config, checkpoint_path=XTTS_CHECKPOINT, vocab_path=TOKENIZER_PATH, use_deepspeed=False)\n",
        "model1.cuda()\n",
        "\n",
        "print(\"Computing speaker latents...\")\n",
        "gpt_cond_latent, speaker_embedding = model1.get_conditioning_latents(audio_path=[SPEAKER_REFERENCE])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5DOniC3FDPT",
        "outputId": "6a922d40-6d36-4c06-d43c-f6ca86f8702c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/TTS/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(f, map_location=map_location, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing speaker latents...\n",
            "Inference...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_WAV_PATH1 = \"Finetuned.wav\"\n"
      ],
      "metadata": {
        "id": "aiFB7lCdFEaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_WAV_PATH = \"Original.wav\""
      ],
      "metadata": {
        "id": "SOSnW1NYGFjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing FineTuned VS Original Model"
      ],
      "metadata": {
        "id": "lb4WAHroH3rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence =  \"الصبح وأنا نازل من البيت لقيت الجو برد جدًا، فاضطريت أرجع أخد جاكيت، ولما وصلت للمحطة اكتشفت إن الميكروباصات كلها زحمة والناس واقفة مستنية\"\n",
        "#Test\n",
        "gpt_cond_latent, speaker_embedding = model.get_conditioning_latents(audio_path=[SPEAKER_REFERENCE])\n",
        "out = model.inference(\n",
        "   sentence,\n",
        "    \"ar\",\n",
        "    gpt_cond_latent,\n",
        "    speaker_embedding,\n",
        "    temperature=0.7, # Add custom parameters here\n",
        ")\n",
        "torchaudio.save(OUTPUT_WAV_PATH, torch.tensor(out[\"wav\"]).unsqueeze(0), 24000)\n",
        "\n",
        "out1 = model1.inference(\n",
        "    sentence,\n",
        "    \"ar\",\n",
        "    gpt_cond_latent,\n",
        "    speaker_embedding,\n",
        "    temperature=0.7, # Add custom parameters here\n",
        ")\n",
        "torchaudio.save(OUTPUT_WAV_PATH1, torch.tensor(out1[\"wav\"]).unsqueeze(0), 24000)\n",
        "\n"
      ],
      "metadata": {
        "id": "ty6JpxbAd-da"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}